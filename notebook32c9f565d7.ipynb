{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==========================================\n# ğŸ“¦ STEP 1: INSTALL & RESTART\n# ==========================================\nimport os, subprocess, sys\n\ndef run_install(pkg):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n\nprint(\"â³ Installing libraries...\")\nrun_install(\"protobuf<=3.20.3\")\nrun_install(\"transformers==4.40.0\")\nrun_install(\"accelerate\")\nrun_install(\"datasets\")\nrun_install(\"sentence-transformers\")\nrun_install(\"hdbscan\")\nrun_install(\"networkx\")\nrun_install(\"py7zr\")\nrun_install(\"rouge_score\")\nrun_install(\"bert_score\")\nrun_install(\"sumy\") # <--- Critical for LexRank/TextRank\nrun_install(\"summac\")\nrun_install(\"git+https://github.com/yuh-zha/AlignScore.git\")\n\nimport nltk\ntry: \n    nltk.download(\"punkt\")\n    nltk.download(\"punkt_tab\")\nexcept: pass\n\nprint(\"âœ… DONE. PLEASE RESTART THE RUNTIME NOW (Runtime > Restart Session).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:47:44.100784Z","iopub.execute_input":"2025-11-28T09:47:44.101312Z","iopub.status.idle":"2025-11-28T09:51:59.267021Z","shell.execute_reply.started":"2025-11-28T09:47:44.101286Z","shell.execute_reply":"2025-11-28T09:51:59.266309Z"}},"outputs":[{"name":"stdout","text":"â³ Installing libraries...\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 162.1/162.1 kB 4.2 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 137.6/137.6 kB 3.3 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9.0/9.0 MB 73.8 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.6/3.6 MB 73.8 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 363.4/363.4 MB 4.4 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13.8/13.8 MB 120.8 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 24.6/24.6 MB 92.4 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 883.7/883.7 kB 44.8 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 664.8/664.8 MB 2.0 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 211.5/211.5 MB 8.9 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 56.3/56.3 MB 34.5 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 127.9/127.9 MB 15.0 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 207.5/207.5 MB 9.1 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 21.1/21.1 MB 98.1 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47.7/47.7 MB 43.8 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.0/44.0 kB 1.4 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12.0/12.0 MB 123.5 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 MB 90.9 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 69.7/69.7 kB 2.7 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 96.4/96.4 kB 5.3 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 50.7/50.7 kB 3.4 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 141.3/141.3 kB 8.1 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 428.8/428.8 kB 16.8 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 61.1/61.1 kB 2.1 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 97.3/97.3 kB 3.8 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6.3/6.3 MB 78.1 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.0/44.0 kB 1.4 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.0/44.0 kB 2.3 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40.1/40.1 kB 2.1 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 42.2/42.2 kB 2.1 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40.1/40.1 kB 2.1 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 42.0/42.0 kB 2.5 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 42.0/42.0 kB 2.6 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 42.0/42.0 kB 2.5 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 42.0/42.0 kB 2.6 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41.7/41.7 kB 2.5 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41.7/41.7 kB 2.5 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40.9/40.9 kB 2.6 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40.9/40.9 kB 2.3 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40.9/40.9 kB 2.5 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40.2/40.2 kB 2.1 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40.2/40.2 kB 2.5 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.0/44.0 kB 2.8 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.4/44.4 kB 2.8 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.4/44.4 kB 2.9 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.4/44.4 kB 2.8 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.4/44.4 kB 3.0 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.1/44.1 kB 2.9 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.5/43.5 kB 2.4 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.1/44.1 kB 2.8 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.1/44.1 kB 2.4 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.1/44.1 kB 2.3 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.4/44.4 kB 2.8 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.4/44.4 kB 2.6 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.4/44.4 kB 2.8 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.7/43.7 kB 2.6 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.7/43.7 kB 2.5 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.7/43.7 kB 2.8 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.7/43.7 kB 2.8 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.7/43.7 kB 2.9 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.7/43.7 kB 2.7 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.7/43.7 kB 2.8 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.7/43.7 kB 2.4 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.6/43.6 kB 2.4 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.6/43.6 kB 2.8 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.6/43.6 kB 2.2 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.6/43.6 kB 2.6 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.6/43.6 kB 1.5 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.8/43.8 kB 2.5 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.8/43.8 kB 2.5 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.8/43.8 kB 2.3 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 138.0/138.0 kB 5.2 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 138.0/138.0 kB 8.5 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 134.8/134.8 kB 7.2 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 134.8/134.8 kB 8.2 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 134.8/134.8 kB 8.5 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 134.8/134.8 kB 8.1 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 130.7/130.7 kB 8.9 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 131.1/131.1 kB 8.1 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 131.1/131.1 kB 9.0 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 129.4/129.4 kB 7.1 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 129.4/129.4 kB 8.6 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 129.4/129.4 kB 8.9 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 126.8/126.8 kB 8.2 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 126.8/126.8 kB 7.8 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 126.8/126.8 kB 7.1 MB/s eta 0:00:00\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 123.5/123.5 kB 6.3 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 294.8/294.8 kB 16.9 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7.9/7.9 MB 94.2 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.6/3.6 MB 97.0 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.4.1 requires huggingface-hub<2.0,>=0.25.0, but you have huggingface-hub 0.17.0 which is incompatible.\ngradio-client 1.11.0 requires huggingface-hub>=0.19.3, but you have huggingface-hub 0.17.0 which is incompatible.\nsentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.17.0 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\ndiffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.17.0 which is incompatible.\npeft 0.16.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.17.0 which is incompatible.\ngradio 5.38.1 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.17.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\naccelerate 1.9.0 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.17.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 527.3/527.3 kB 10.3 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 162.1/162.1 kB 11.0 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 829.5/829.5 kB 29.0 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 887.4/887.4 MB 2.0 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 317.1/317.1 MB 1.9 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 21.0/21.0 MB 100.9 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 849.3/849.3 kB 48.8 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 557.1/557.1 MB 3.4 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 116.3/116.3 kB 8.1 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 177.6/177.6 kB 11.5 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 566.1/566.1 kB 33.6 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4.1/4.1 MB 105.4 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 891.4/891.4 kB 43.8 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 143.5/143.5 kB 10.1 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nsummac 0.0.4 requires huggingface-hub<=0.17.0, but you have huggingface-hub 0.36.0 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-secret-manager 2.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-vision 3.11.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-monitoring 2.28.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-videointelligence 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2024.6.1 which is incompatible.\ngoogle-api-core 2.28.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.0 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nray 2.51.1 requires protobuf>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-bigtable 2.34.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-discoveryengine 0.13.12 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-aiplatform 1.125.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-speech 2.34.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-logging 3.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-trace 1.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-appengine-logging 1.7.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-audit-log 0.4.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-cloud-functions 1.20.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-resource-manager 1.14.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngoogle-cloud-dataproc 5.21.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ntorchdata 0.11.0 requires torch>=2, but you have torch 1.13.1 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.0 which is incompatible.\ngoogleapis-common-protos 1.70.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ngoogle-cloud-datastore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.0 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\ngoogle-cloud-iam 2.19.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-language 2.17.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-bigquery-connection 1.18.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngrpc-google-iam-v1 0.14.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\naccelerate 1.9.0 requires torch>=2.0.0, but you have torch 1.13.1 which is incompatible.\ngoogle-cloud-spanner 3.56.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-firestore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"âœ… DONE. PLEASE RESTART THE RUNTIME NOW (Runtime > Restart Session).\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nHMTAS-full\nResearch-Grade Pipeline: HMTAS vs. DL Models vs. Extractive Baselines\n\n- INCLUDES: BART, PEGASUS, FLAN-T5 (Direct & HMTAS)\n- INCLUDES: LexRank & TextRank (Baselines)\n- METRICS: ROUGE-1/2/L, BERTScore, Summac, AlignScore\n- HARDWARE: Multi-GPU Support (Round-Robin), Memory Optimized\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport sys\nfrom pathlib import Path\n\n# ==========================================\n# 0. CRITICAL ENVIRONMENT FIXES\n# ==========================================\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"JAX_PLATFORMS\"] = \"cpu\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"HF_HOME\"] = str(Path(\"./hf_cache\").resolve())\nPath(os.environ[\"HF_HOME\"]).mkdir(parents=True, exist_ok=True)\n\n# ---------------------------\n# Imports\n# ---------------------------\nimport time\nimport json\nimport math\nimport random\nimport logging\nimport argparse\nimport warnings\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport nltk\nfrom typing import List, Dict, Optional, Any\n\n# ---------------------------\n# Logging Setup\n# ---------------------------\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nlogger = logging.getLogger(\"hmtas_mode_c\")\nlogger.setLevel(logging.INFO)\nif not logger.handlers:\n    ch = logging.StreamHandler()\n    ch.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n    logger.addHandler(ch)\n\n# ---------------------------\n# Safe Imports\n# ---------------------------\ntry:\n    from datasets import load_dataset\nexcept ImportError: load_dataset = None\ntry:\n    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, logging as hf_logging\n    hf_logging.set_verbosity_error()\nexcept ImportError: AutoTokenizer = None\ntry:\n    from sentence_transformers import SentenceTransformer, util as st_util\nexcept ImportError: SentenceTransformer = None\ntry:\n    from rouge_score import rouge_scorer\nexcept ImportError: rouge_scorer = None\ntry:\n    from bert_score import score as bert_score\nexcept ImportError: bert_score = None\n\n# Extractive Summarizers\ntry:\n    from sumy.parsers.plaintext import PlaintextParser\n    from sumy.nlp.tokenizers import Tokenizer\n    from sumy.summarizers.lex_rank import LexRankSummarizer\n    from sumy.summarizers.text_rank import TextRankSummarizer\nexcept ImportError:\n    logger.warning(\"sumy not installed. Baselines will be skipped.\")\n    LexRankSummarizer = None\n\n# ---------------------------\n# Configuration\n# ---------------------------\nclass Config:\n    dataset_samples: int = 400\n    seed = 42\n    \n    use_gpus: bool = torch.cuda.is_available()\n    # Detect all available GPUs\n    devices: List[str] = [f\"cuda:{i}\" for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else [\"cpu\"]\n\n    MODEL_REG = {\n        \"english\": {\n            \"FLAN-T5-Base\":   {\"name\": \"google/flan-t5-base\", \"ctx\": 512},\n            \"BART-Large-CNN\": {\"name\": \"facebook/bart-large-cnn\", \"ctx\": 1024},\n            \"PEGASUS-Large\":  {\"name\": \"google/pegasus-large\", \"ctx\": 1024},\n        }\n    }\n\n    gen_num_beams = 4\n    gen_batch_gpu = 8 # Higher batch size for T4s\n    encoder_name = \"paraphrase-multilingual-mpnet-base-v2\"\n    nli_model_name = \"roberta-large-mnli\"\n    \n    # Metric Toggles (Must exist to avoid AttributeError)\n    do_bertscore = True\n    do_summac = True\n    do_align = True\n    \n    out_dir = Path(\"./hmtas_mode_c_output\")\n    cache_dir = os.environ[\"HF_HOME\"]\n\ncfg = Config()\ncfg.out_dir.mkdir(parents=True, exist_ok=True)\n\n# ---------------------------\n# Utilities\n# ---------------------------\nSEP = \"\\n\\n---\\n\\n\"\n\ndef clean_gpu():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\ndef split_sentences(text: str) -> List[str]:\n    if not isinstance(text, str): return []\n    text = text.replace(\"\\n\", \" \").strip()\n    return [s.strip() for s in text.split(\".\") if len(s.strip().split()) > 4]\n\nclass SentenceEncoder:\n    def __init__(self, model_name: str = cfg.encoder_name, device: str = \"cpu\"):\n        self.model = SentenceTransformer(model_name, device=device, cache_folder=cfg.cache_dir)\n    def encode(self, texts: List[str]):\n        if not texts: return np.zeros((0, 384))\n        return self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n\ndef pack_articles(docs: List[str], prefix: str = \"Summarize: \") -> str:\n    return (prefix + SEP.join(docs))[:4000]\n\ndef create_guided_input(docs: List[str], encoder: Optional[SentenceEncoder]) -> str:\n    if encoder is None: return pack_articles(docs)\n    all_sents = []\n    for d in docs: all_sents.extend(split_sentences(d))\n    if not all_sents: return pack_articles(docs)\n    try:\n        emb = encoder.encode(all_sents[:50]) \n        sim = np.dot(emb, emb.T)\n        centrality = sim.sum(axis=1)\n        top_idx = sorted(np.argsort(-centrality)[:5])\n        kp = \"\\n- \".join([all_sents[i] for i in top_idx])\n    except: kp = \"\\n- \".join(all_sents[:3])\n    return (f\"Key points:\\n{kp}\\n\\nTask: Summarize.\\n\\n\" + SEP.join(docs))[:4000]\n\n# ---------------------------\n# Generation (Deep Learning)\n# ---------------------------\ndef safe_generate_batch(model, tokenizer, texts, device):\n    outs = []\n    model.eval()\n    bs = cfg.gen_batch_gpu\n    vocab_limit = model.config.vocab_size if hasattr(model, \"config\") else 32000\n    for i in range(0, len(texts), bs):\n        batch = texts[i:i+bs]\n        try:\n            inputs = tokenizer(batch, truncation=True, padding=True, max_length=512, return_tensors=\"pt\").to(device)\n            inputs[\"input_ids\"][inputs[\"input_ids\"] >= vocab_limit] = tokenizer.pad_token_id\n            with torch.no_grad():\n                gen_ids = model.generate(**inputs, max_length=256, min_length=20, num_beams=2)\n            outs.extend(tokenizer.batch_decode(gen_ids, skip_special_tokens=True))\n        except Exception:\n            outs.extend([\"\"] * len(batch))\n            torch.cuda.empty_cache()\n    return outs\n\n# ---------------------------\n# Generation (Extractive Baselines)\n# ---------------------------\ndef generate_extractive(texts: List[str], method=\"lexrank\", sentences_count=3) -> List[str]:\n    if LexRankSummarizer is None: return [\"\"] * len(texts)\n    summaries = []\n    \n    # Initialize\n    if method == \"lexrank\": summarizer = LexRankSummarizer()\n    else: summarizer = TextRankSummarizer()\n        \n    for text in texts:\n        try:\n            parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n            sents = summarizer(parser.document, sentences_count)\n            summaries.append(\" \".join([str(s) for s in sents]))\n        except:\n            summaries.append(\"\")\n    return summaries\n\n# ---------------------------\n# Metric Functions (Optimized)\n# ---------------------------\ndef run_summac_fast(src, cand, model, tokenizer, device):\n    if not src or not cand: return 0.0\n    s_sents = split_sentences(cand)\n    d_sents = split_sentences(src)[:20]\n    if not s_sents or not d_sents: return 0.0\n    try:\n        pairs = [(d_sents, s) for s in s_sents[:3]] \n        doc_repeats, hyp_repeats = [], []\n        for d, h in pairs:\n            doc_repeats.extend(d); hyp_repeats.extend([h] * len(d))\n        if not doc_repeats: return 0.0\n        inputs = tokenizer(doc_repeats, hyp_repeats, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n        with torch.no_grad():\n            logits = model(**inputs).logits\n            probs = torch.softmax(logits, dim=-1)\n        entailment_probs = probs[:, 2].view(len(pairs), -1)\n        return float(entailment_probs.max(dim=1).values.mean().item())\n    except: return 0.0\n\ndef run_align_fast(src, cand, encoder):\n    try:\n        e1 = encoder.encode(split_sentences(src))\n        e2 = encoder.encode(split_sentences(cand))\n        if len(e1) == 0 or len(e2) == 0: return 0.0\n        sim = np.dot(e2, e1.T)\n        return float(sim.max(axis=1).mean())\n    except: return 0.0\n\n# ---------------------------\n# Main Logic\n# ---------------------------\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--split\", default=\"test\")\n    parser.add_argument(\"--samples\", type=int, default=cfg.dataset_samples)\n    parser.add_argument(\"--no-heavy\", action=\"store_true\")\n    args, _ = parser.parse_known_args()\n\n    if args.no_heavy: cfg.do_summac = cfg.do_align = cfg.do_bertscore = False\n\n    logger.info(f\"ğŸš€ Starting Pipeline (Samples: {cfg.dataset_samples})\")\n    logger.info(f\"   Using Devices: {cfg.devices}\")\n    \n    # 1. LOAD DATA\n    pool = [{\"texts\": [\"Text\"]*10, \"summary\": \"Sum\"}] * cfg.dataset_samples\n    if load_dataset:\n        try:\n            ds = load_dataset(\"Awesome075/multi_news_parquet\", split=args.split, cache_dir=cfg.cache_dir)\n            pool = []\n            for item in ds:\n                docs = [d.strip() for d in item[\"document\"].split(\"||||\") if len(d) > 50]\n                if docs:\n                    pool.append({\"texts\": docs, \"summary\": item[\"summary\"]})\n                    if len(pool) >= cfg.dataset_samples: break\n        except: pass\n\n    results_store = {}\n    \n    encoder = None\n    if SentenceTransformer: encoder = SentenceEncoder(device=\"cpu\")\n\n    # 2. DEEP LEARNING GEN (MULTI-GPU ROUND ROBIN)\n    model_items = list(cfg.MODEL_REG[\"english\"].items())\n    \n    for i, (m_name, m_spec) in enumerate(model_items):\n        # Assign to GPU 0 or GPU 1 dynamically\n        device = cfg.devices[i % len(cfg.devices)]\n        logger.info(f\"--> Generating DL: {m_name} on {device}\")\n        \n        clean_gpu()\n        try:\n            tok = AutoTokenizer.from_pretrained(m_spec[\"name\"], cache_dir=cfg.cache_dir)\n            mod = AutoModelForSeq2SeqLM.from_pretrained(m_spec[\"name\"], cache_dir=cfg.cache_dir).to(device)\n            if cfg.use_gpus: mod.half()\n            \n            srcs = [\" \".join(p[\"texts\"]) for p in pool]\n            refs = [p[\"summary\"] for p in pool]\n            \n            ins_d = [pack_articles(p[\"texts\"]) for p in pool]\n            ins_h = [create_guided_input(p[\"texts\"], encoder) for p in pool]\n            \n            preds_d = safe_generate_batch(mod, tok, ins_d, device)\n            preds_h = safe_generate_batch(mod, tok, ins_h, device)\n            \n            results_store[m_name] = { \"src\": srcs, \"ref\": refs, \"pred_d\": preds_d, \"pred_h\": preds_h }\n            del mod; clean_gpu()\n        except Exception as e:\n            logger.error(f\"{m_name} failed: {e}\")\n\n    # 3. BASELINE GEN (LexRank/TextRank) - Runs on CPU\n    if LexRankSummarizer:\n        logger.info(\"--> Generating Baselines...\")\n        srcs_full = [\" \".join(p[\"texts\"]) for p in pool]\n        refs = [p[\"summary\"] for p in pool]\n        \n        lex_preds = generate_extractive(srcs_full, \"lexrank\")\n        text_preds = generate_extractive(srcs_full, \"textrank\")\n        \n        # Use same format as DL models (d=h for baselines)\n        results_store[\"Baseline-LexRank\"] = { \"src\": srcs_full, \"ref\": refs, \"pred_d\": lex_preds, \"pred_h\": lex_preds }\n        results_store[\"Baseline-TextRank\"] = { \"src\": srcs_full, \"ref\": refs, \"pred_d\": text_preds, \"pred_h\": text_preds }\n\n    # 4. EVALUATION\n    logger.info(\"Starting Evaluation...\")\n    \n    # Use GPU 0 for evaluation to keep logic simple\n    eval_device = cfg.devices[0]\n    \n    summac_model = None\n    summac_tok = None\n    if cfg.do_summac:\n        try:\n            from transformers import AutoModelForSequenceClassification\n            summac_tok = AutoTokenizer.from_pretrained(cfg.nli_model_name)\n            summac_model = AutoModelForSequenceClassification.from_pretrained(cfg.nli_model_name).to(eval_device)\n            if cfg.use_gpus: summac_model.half()\n        except: cfg.do_summac = False\n\n    for m_name, res in results_store.items():\n        logger.info(f\"Evaluating {m_name}...\")\n        \n        loop_modes = [\"d\", \"h\"] if \"Baseline\" not in m_name else [\"d\"]\n        \n        for mode in loop_modes:\n            preds = res[f\"pred_{mode}\"]\n            refs = res[\"ref\"]\n            srcs = res[\"src\"]\n            \n            # ROUGE\n            r_res = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n            if rouge_scorer:\n                scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n                for r, p in zip(refs, preds):\n                    try:\n                        s = scorer.score(r, p)\n                        r_res[\"rouge1\"].append(s[\"rouge1\"].fmeasure)\n                        r_res[\"rouge2\"].append(s[\"rouge2\"].fmeasure)\n                        r_res[\"rougeL\"].append(s[\"rougeL\"].fmeasure)\n                    except:\n                        for k in r_res: r_res[k].append(0.0)\n            \n            # BERTScore (Explicit clean before/after)\n            bs_scores = [0.0]*len(preds)\n            if cfg.do_bertscore and bert_score:\n                try:\n                    clean_gpu()\n                    _, _, F = bert_score(preds, refs, lang=\"en\", device=eval_device, verbose=False, batch_size=4)\n                    bs_scores = F.tolist()\n                    clean_gpu()\n                except: pass\n            \n            # Heavy Metrics\n            sm_scores, al_scores = [], []\n            for i, (src, cand) in enumerate(zip(srcs, preds)):\n                if i % 10 == 0: clean_gpu()\n                if cfg.do_summac and summac_model:\n                    sm_scores.append(run_summac_fast(src, cand, summac_model, summac_tok, eval_device))\n                if cfg.do_align and encoder:\n                    al_scores.append(run_align_fast(src, cand, encoder))\n\n            # Save\n            df = pd.DataFrame({\n                \"ref\": refs, \"pred\": preds,\n                \"rouge1\": r_res[\"rouge1\"], \"rouge2\": r_res[\"rouge2\"], \"rougeL\": r_res[\"rougeL\"],\n                \"bertscore\": bs_scores, \"summac\": sm_scores, \"align\": al_scores\n            })\n            df.to_csv(cfg.out_dir / f\"{m_name}_{mode}_results.csv\", index=False)\n\n    if summac_model: del summac_model\n    clean_gpu()\n    logger.info(f\"âœ… Done! Results saved to {cfg.out_dir}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:53:46.646546Z","iopub.execute_input":"2025-11-28T09:53:46.647171Z","iopub.status.idle":"2025-11-28T13:37:30.156688Z","shell.execute_reply.started":"2025-11-28T09:53:46.647145Z","shell.execute_reply":"2025-11-28T13:37:30.155972Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764323642.192398      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764323642.248497      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"2025-11-28 09:54:22,152 - WARNING - sumy not installed. Baselines will be skipped.\nWARNING:hmtas_mode_c:sumy not installed. Baselines will be skipped.\n2025-11-28 09:54:22,157 - INFO - ğŸš€ Starting Pipeline (Samples: 400)\nINFO:hmtas_mode_c:ğŸš€ Starting Pipeline (Samples: 400)\n2025-11-28 09:54:22,159 - INFO -    Using Devices: ['cuda:0', 'cuda:1']\nINFO:hmtas_mode_c:   Using Devices: ['cuda:0', 'cuda:1']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdf6454cf3c84c86a949b70dfc09b8e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.parquet:   0%|          | 0.00/323M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"828f74593c7c4d33856a35d6f677c2d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.parquet:   0%|          | 0.00/39.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb89b2ad20d34f3eb367ff2843fa22aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.parquet:   0%|          | 0.00/40.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0723351578834a5283341d201a523e4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/44972 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ac20442f2fa42808e879f33b4b676e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/5622 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"349aef8c319e48739ab311ff0dc8c95d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5622 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71e0cf79b6144653a4063fac555ef818"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb1c0e4d65e84ac78fcb19648cdd4e8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b8e173860734c88b02d904b85cf52b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eefea824b4a447cf8dec6e80e3c2c4b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"893b3f7493e54dc1a6762dddfb117d2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e963bff41aa04f2bb3aa9fc95e5335c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdeed45761af428d8148354f6cc33e68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01e9f31c685b498182f6cedbd01416c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f27d804a2b142129b557abc64132995"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7199ed68f3949d4887be28da9c3062d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"133f8dcf7a164963a5fde848e5e932bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827e96fef27544d098455e6f770f34ed"}},"metadata":{}},{"name":"stderr","text":"2025-11-28 09:54:40,774 - INFO - --> Generating DL: FLAN-T5-Base on cuda:0\nINFO:hmtas_mode_c:--> Generating DL: FLAN-T5-Base on cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"434ffa24b84a4ebdb9f8c9e5179a09d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c49ba5dae03f48d5a599c0f0670ce65d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43a2e771bbb943b5a858cd239d426ebe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"710885b01c114af282d669f7754e6552"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4215a57ac37f43acb07b53264a71af4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"583be9d3dfc747408fe6d66c9bf4bbd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ae5fcefcc1490e8bfa92f64af7448c"}},"metadata":{}},{"name":"stderr","text":"2025-11-28 10:22:44,881 - INFO - --> Generating DL: BART-Large-CNN on cuda:1\nINFO:hmtas_mode_c:--> Generating DL: BART-Large-CNN on cuda:1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"595b0c07ea3b496ba72f7e771fca14f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfec7c325eaa4cfe965ddf3b0c524728"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7da60918491445f0a59e75afb818d7ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb0ce7f73a5542b8ae45233398369b21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adcab104f6994a30a1f59cbf10211f78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b53c609c1bc46e89f999a17de2bd3d7"}},"metadata":{}},{"name":"stderr","text":"2025-11-28 10:39:54,700 - INFO - --> Generating DL: PEGASUS-Large on cuda:0\nINFO:hmtas_mode_c:--> Generating DL: PEGASUS-Large on cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d15aa631a7e0457f872e70c2bf6d92fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb1e4de436243219f6f09cef0afbc23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b98a87c6ddf4e1093bae33bd2f2bfaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14d3fb16f86c4db0af93f193e5df5d36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e5de8a72a3b460a8fa5504002293c3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7603632e4b4549a9a330be2f90a76568"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cd2f61b72f74830b2ba3e029c7ed3da"}},"metadata":{}},{"name":"stderr","text":"2025-11-28 11:04:26,689 - INFO - Starting Evaluation...\nINFO:hmtas_mode_c:Starting Evaluation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cff2cae0171847c0b1dd667b3b5675d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b6456a6550f4786adde682d86ec0d48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eeb99e6a6fb4a1db48b2fec4f0a6332"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c0c7719d5a3454c8ce17ce1af0ae6d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11f7aa08209645a59465aff28e29485f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3e02d7e0a5c41a5bc5d8f0fe4dda2e9"}},"metadata":{}},{"name":"stderr","text":"2025-11-28 11:04:32,711 - INFO - Evaluating FLAN-T5-Base...\nINFO:hmtas_mode_c:Evaluating FLAN-T5-Base...\n2025-11-28 11:56:56,076 - INFO - Evaluating BART-Large-CNN...\nINFO:hmtas_mode_c:Evaluating BART-Large-CNN...\n2025-11-28 12:46:42,529 - INFO - Evaluating PEGASUS-Large...\nINFO:hmtas_mode_c:Evaluating PEGASUS-Large...\n2025-11-28 13:37:30,067 - INFO - âœ… Done! Results saved to hmtas_mode_c_output\nINFO:hmtas_mode_c:âœ… Done! Results saved to hmtas_mode_c_output\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==========================================\n# ğŸ“Š HMTAS FINAL REPORT: PUBLICATION READY\n# ==========================================\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom pathlib import Path\nfrom IPython.display import display, HTML, Markdown\n\n# --- Config ---\nRESULTS_DIR = Path(\"/kaggle/working/hmtas_mode_c_output\")\nSHOW_N_SAMPLES = 2\n\ndef get_metric_cols(df):\n    exclude = ['ref', 'pred', 'sources', 'src', 'model_name', 'Unnamed: 0', 'id']\n    return [c for c in df.columns if c not in exclude]\n\ndef color_delta(val):\n    if abs(val) < 0.0001: return 'color: #bdc3c7'\n    color = '#27ae60' if val > 0 else '#c0392b'\n    weight = 'bold' if abs(val) > 0 else 'normal'\n    return f'color: {color}; font-weight: {weight}'\n\ndef analyze_results():\n    if not RESULTS_DIR.exists():\n        print(\"âŒ Output directory not found.\")\n        return\n\n    print(f\"ğŸ“‚ Loading results from: {RESULTS_DIR.resolve()} ...\\n\")\n\n    files = list(RESULTS_DIR.glob(\"*.csv\"))\n    models = set()\n    for f in files:\n        if \"wrapper\" in f.name or \"config\" in f.name: continue\n        name_part = f.name.rsplit('_', 2)[0]\n        models.add(name_part)\n\n    if not models:\n        print(\"âš ï¸ No results found.\")\n        return\n\n    # Container for DL models (HMTAS vs Direct)\n    dl_stats = []\n    # Container for Baselines (Single scores)\n    baseline_stats = []\n\n    for model in sorted(list(models)):\n        path_d = RESULTS_DIR / f\"{model}_d_results.csv\"\n        path_h = RESULTS_DIR / f\"{model}_h_results.csv\"\n        \n        if not path_d.exists(): continue\n        # If no HMTAS file exists (rare), fallback to Direct\n        if not path_h.exists(): path_h = path_d \n\n        df_d = pd.read_csv(path_d)\n        df_h = pd.read_csv(path_h)\n        \n        metric_cols = get_metric_cols(df_d)\n        # Force numeric\n        for c in metric_cols:\n            df_d[c] = pd.to_numeric(df_d[c], errors='coerce')\n            df_h[c] = pd.to_numeric(df_h[c], errors='coerce')\n\n        min_len = min(len(df_d), len(df_h))\n        df_d = df_d.iloc[:min_len]\n        df_h = df_h.iloc[:min_len]\n\n        # SEPARATE LOGIC: DL vs BASELINE\n        if \"Baseline\" in model:\n            # For baselines, we just want the raw score\n            for metric in metric_cols:\n                score = df_d[metric].mean()\n                baseline_stats.append({\n                    \"Method\": model.replace(\"Baseline-\", \"\"),\n                    \"Metric\": metric,\n                    \"Score\": score\n                })\n        else:\n            # For DL models, we calculate the Gain/Delta\n            for metric in metric_cols:\n                if metric not in df_h.columns: continue\n                mu_base = df_d[metric].mean()\n                mu_hmtas = df_h[metric].mean()\n                \n                wins = (df_h[metric] > df_d[metric]).sum()\n                win_rate = (wins / min_len) * 100.0\n                delta = mu_hmtas - mu_base\n                pct_gain = (delta / (abs(mu_base) + 1e-9)) * 100.0\n\n                dl_stats.append({\n                    \"Model\": model,\n                    \"Metric\": metric,\n                    \"Base\": mu_base,\n                    \"HMTAS\": mu_hmtas,\n                    \"Gain (%)\": pct_gain,\n                    \"Win Rate (%)\": win_rate\n                })\n\n    # --- TABLE 1: HMTAS IMPACT (DL Models Only) ---\n    if dl_stats:\n        df_dl = pd.DataFrame(dl_stats)\n        priority = ['rouge1', 'rouge2', 'rougeL', 'bertscore', 'summac', 'align']\n        df_dl['Metric'] = pd.Categorical(df_dl['Metric'], categories=priority + [c for c in df_dl['Metric'].unique() if c not in priority], ordered=True)\n        df_dl = df_dl.sort_values(['Model', 'Metric'])\n\n        display(Markdown(\"## ğŸ“Š 1. HMTAS Impact Study (Deep Learning Models)\"))\n        display(Markdown(\"*Shows how much HMTAS improved the base model.*\"))\n        \n        styler = df_dl.style.format({\n            \"Base\": \"{:.4f}\", \"HMTAS\": \"{:.4f}\", \"Gain (%)\": \"{:+.2f}%\", \"Win Rate (%)\": \"{:.1f}%\"\n        }).applymap(color_delta, subset=[\"Gain (%)\"]).hide(axis='index')\n        display(styler)\n\n    # --- TABLE 2: BENCHMARKING (Best HMTAS vs Baselines) ---\n    if baseline_stats and dl_stats:\n        # Get average HMTAS score for each model to find the \"Champion\"\n        best_hmtas = pd.DataFrame(dl_stats).groupby(['Model', 'Metric'])['HMTAS'].mean().reset_index()\n        df_base = pd.DataFrame(baseline_stats)\n        \n        # Combine for comparison\n        comparison = []\n        metrics = df_base['Metric'].unique()\n        \n        for m in metrics:\n            # Get baselines\n            row = {\"Metric\": m}\n            for _, b_row in df_base[df_base['Metric'] == m].iterrows():\n                row[b_row['Method']] = b_row['Score']\n            \n            # Get DL Models (HMTAS version)\n            for _, d_row in best_hmtas[best_hmtas['Metric'] == m].iterrows():\n                row[d_row['Model']] = d_row['HMTAS']\n            \n            comparison.append(row)\n            \n        df_comp = pd.DataFrame(comparison)\n        df_comp['Metric'] = pd.Categorical(df_comp['Metric'], categories=priority + [c for c in df_comp['Metric'].unique() if c not in priority], ordered=True)\n        df_comp = df_comp.sort_values('Metric')\n\n        display(Markdown(\"## ğŸ† 2. Final Benchmark: HMTAS vs. Extractive Baselines\"))\n        display(Markdown(\"*Comparing the final HMTAS output against standard LexRank/TextRank.*\"))\n        \n        # Highlight best score in each row\n        def highlight_max(s):\n            is_max = s == s.max()\n            return ['background-color: #d4efdf; font-weight: bold' if v else '' for v in is_max]\n\n        styler_comp = df_comp.set_index(\"Metric\").style.format(\"{:.4f}\").apply(highlight_max, axis=1)\n        display(styler_comp)\n\nanalyze_results()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:13:13.569375Z","iopub.execute_input":"2025-11-28T14:13:13.569823Z","iopub.status.idle":"2025-11-28T14:13:13.760409Z","shell.execute_reply.started":"2025-11-28T14:13:13.569784Z","shell.execute_reply":"2025-11-28T14:13:13.759812Z"}},"outputs":[{"name":"stdout","text":"ğŸ“‚ Loading results from: /kaggle/working/hmtas_mode_c_output ...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## ğŸ“Š 1. HMTAS Impact Study (Deep Learning Models)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"*Shows how much HMTAS improved the base model.*"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7bc0255cc950>","text/html":"<style type=\"text/css\">\n#T_ab528_row0_col4, #T_ab528_row1_col4, #T_ab528_row2_col4, #T_ab528_row5_col4, #T_ab528_row6_col4, #T_ab528_row7_col4, #T_ab528_row8_col4, #T_ab528_row10_col4, #T_ab528_row11_col4, #T_ab528_row12_col4, #T_ab528_row13_col4, #T_ab528_row14_col4 {\n  color: #27ae60;\n  font-weight: bold;\n}\n#T_ab528_row3_col4, #T_ab528_row9_col4, #T_ab528_row15_col4 {\n  color: #bdc3c7;\n}\n#T_ab528_row4_col4, #T_ab528_row16_col4, #T_ab528_row17_col4 {\n  color: #c0392b;\n  font-weight: bold;\n}\n</style>\n<table id=\"T_ab528\">\n  <thead>\n    <tr>\n      <th id=\"T_ab528_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n      <th id=\"T_ab528_level0_col1\" class=\"col_heading level0 col1\" >Metric</th>\n      <th id=\"T_ab528_level0_col2\" class=\"col_heading level0 col2\" >Base</th>\n      <th id=\"T_ab528_level0_col3\" class=\"col_heading level0 col3\" >HMTAS</th>\n      <th id=\"T_ab528_level0_col4\" class=\"col_heading level0 col4\" >Gain (%)</th>\n      <th id=\"T_ab528_level0_col5\" class=\"col_heading level0 col5\" >Win Rate (%)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_ab528_row0_col0\" class=\"data row0 col0\" >BART-Large-CNN</td>\n      <td id=\"T_ab528_row0_col1\" class=\"data row0 col1\" >rouge1</td>\n      <td id=\"T_ab528_row0_col2\" class=\"data row0 col2\" >0.2199</td>\n      <td id=\"T_ab528_row0_col3\" class=\"data row0 col3\" >0.2375</td>\n      <td id=\"T_ab528_row0_col4\" class=\"data row0 col4\" >+8.01%</td>\n      <td id=\"T_ab528_row0_col5\" class=\"data row0 col5\" >61.5%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row1_col0\" class=\"data row1 col0\" >BART-Large-CNN</td>\n      <td id=\"T_ab528_row1_col1\" class=\"data row1 col1\" >rouge2</td>\n      <td id=\"T_ab528_row1_col2\" class=\"data row1 col2\" >0.0772</td>\n      <td id=\"T_ab528_row1_col3\" class=\"data row1 col3\" >0.0837</td>\n      <td id=\"T_ab528_row1_col4\" class=\"data row1 col4\" >+8.36%</td>\n      <td id=\"T_ab528_row1_col5\" class=\"data row1 col5\" >48.8%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row2_col0\" class=\"data row2 col0\" >BART-Large-CNN</td>\n      <td id=\"T_ab528_row2_col1\" class=\"data row2 col1\" >rougeL</td>\n      <td id=\"T_ab528_row2_col2\" class=\"data row2 col2\" >0.1391</td>\n      <td id=\"T_ab528_row2_col3\" class=\"data row2 col3\" >0.1480</td>\n      <td id=\"T_ab528_row2_col4\" class=\"data row2 col4\" >+6.38%</td>\n      <td id=\"T_ab528_row2_col5\" class=\"data row2 col5\" >53.2%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row3_col0\" class=\"data row3 col0\" >BART-Large-CNN</td>\n      <td id=\"T_ab528_row3_col1\" class=\"data row3 col1\" >bertscore</td>\n      <td id=\"T_ab528_row3_col2\" class=\"data row3 col2\" >0.0000</td>\n      <td id=\"T_ab528_row3_col3\" class=\"data row3 col3\" >0.0000</td>\n      <td id=\"T_ab528_row3_col4\" class=\"data row3 col4\" >+0.00%</td>\n      <td id=\"T_ab528_row3_col5\" class=\"data row3 col5\" >0.0%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row4_col0\" class=\"data row4 col0\" >BART-Large-CNN</td>\n      <td id=\"T_ab528_row4_col1\" class=\"data row4 col1\" >summac</td>\n      <td id=\"T_ab528_row4_col2\" class=\"data row4 col2\" >0.8200</td>\n      <td id=\"T_ab528_row4_col3\" class=\"data row4 col3\" >0.7797</td>\n      <td id=\"T_ab528_row4_col4\" class=\"data row4 col4\" >-4.91%</td>\n      <td id=\"T_ab528_row4_col5\" class=\"data row4 col5\" >43.2%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row5_col0\" class=\"data row5 col0\" >BART-Large-CNN</td>\n      <td id=\"T_ab528_row5_col1\" class=\"data row5 col1\" >align</td>\n      <td id=\"T_ab528_row5_col2\" class=\"data row5 col2\" >7.2849</td>\n      <td id=\"T_ab528_row5_col3\" class=\"data row5 col3\" >7.5059</td>\n      <td id=\"T_ab528_row5_col4\" class=\"data row5 col4\" >+3.03%</td>\n      <td id=\"T_ab528_row5_col5\" class=\"data row5 col5\" >60.5%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row6_col0\" class=\"data row6 col0\" >FLAN-T5-Base</td>\n      <td id=\"T_ab528_row6_col1\" class=\"data row6 col1\" >rouge1</td>\n      <td id=\"T_ab528_row6_col2\" class=\"data row6 col2\" >0.3265</td>\n      <td id=\"T_ab528_row6_col3\" class=\"data row6 col3\" >0.3528</td>\n      <td id=\"T_ab528_row6_col4\" class=\"data row6 col4\" >+8.06%</td>\n      <td id=\"T_ab528_row6_col5\" class=\"data row6 col5\" >56.8%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row7_col0\" class=\"data row7 col0\" >FLAN-T5-Base</td>\n      <td id=\"T_ab528_row7_col1\" class=\"data row7 col1\" >rouge2</td>\n      <td id=\"T_ab528_row7_col2\" class=\"data row7 col2\" >0.1053</td>\n      <td id=\"T_ab528_row7_col3\" class=\"data row7 col3\" >0.1189</td>\n      <td id=\"T_ab528_row7_col4\" class=\"data row7 col4\" >+13.00%</td>\n      <td id=\"T_ab528_row7_col5\" class=\"data row7 col5\" >55.2%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row8_col0\" class=\"data row8 col0\" >FLAN-T5-Base</td>\n      <td id=\"T_ab528_row8_col1\" class=\"data row8 col1\" >rougeL</td>\n      <td id=\"T_ab528_row8_col2\" class=\"data row8 col2\" >0.1862</td>\n      <td id=\"T_ab528_row8_col3\" class=\"data row8 col3\" >0.1932</td>\n      <td id=\"T_ab528_row8_col4\" class=\"data row8 col4\" >+3.76%</td>\n      <td id=\"T_ab528_row8_col5\" class=\"data row8 col5\" >50.0%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row9_col0\" class=\"data row9 col0\" >FLAN-T5-Base</td>\n      <td id=\"T_ab528_row9_col1\" class=\"data row9 col1\" >bertscore</td>\n      <td id=\"T_ab528_row9_col2\" class=\"data row9 col2\" >0.0000</td>\n      <td id=\"T_ab528_row9_col3\" class=\"data row9 col3\" >0.0000</td>\n      <td id=\"T_ab528_row9_col4\" class=\"data row9 col4\" >+0.00%</td>\n      <td id=\"T_ab528_row9_col5\" class=\"data row9 col5\" >0.0%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row10_col0\" class=\"data row10 col0\" >FLAN-T5-Base</td>\n      <td id=\"T_ab528_row10_col1\" class=\"data row10 col1\" >summac</td>\n      <td id=\"T_ab528_row10_col2\" class=\"data row10 col2\" >0.7225</td>\n      <td id=\"T_ab528_row10_col3\" class=\"data row10 col3\" >0.7559</td>\n      <td id=\"T_ab528_row10_col4\" class=\"data row10 col4\" >+4.62%</td>\n      <td id=\"T_ab528_row10_col5\" class=\"data row10 col5\" >41.5%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row11_col0\" class=\"data row11 col0\" >FLAN-T5-Base</td>\n      <td id=\"T_ab528_row11_col1\" class=\"data row11 col1\" >align</td>\n      <td id=\"T_ab528_row11_col2\" class=\"data row11 col2\" >7.4411</td>\n      <td id=\"T_ab528_row11_col3\" class=\"data row11 col3\" >7.9301</td>\n      <td id=\"T_ab528_row11_col4\" class=\"data row11 col4\" >+6.57%</td>\n      <td id=\"T_ab528_row11_col5\" class=\"data row11 col5\" >64.0%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row12_col0\" class=\"data row12 col0\" >PEGASUS-Large</td>\n      <td id=\"T_ab528_row12_col1\" class=\"data row12 col1\" >rouge1</td>\n      <td id=\"T_ab528_row12_col2\" class=\"data row12 col2\" >0.3237</td>\n      <td id=\"T_ab528_row12_col3\" class=\"data row12 col3\" >0.3724</td>\n      <td id=\"T_ab528_row12_col4\" class=\"data row12 col4\" >+15.05%</td>\n      <td id=\"T_ab528_row12_col5\" class=\"data row12 col5\" >71.8%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row13_col0\" class=\"data row13 col0\" >PEGASUS-Large</td>\n      <td id=\"T_ab528_row13_col1\" class=\"data row13 col1\" >rouge2</td>\n      <td id=\"T_ab528_row13_col2\" class=\"data row13 col2\" >0.1063</td>\n      <td id=\"T_ab528_row13_col3\" class=\"data row13 col3\" >0.1126</td>\n      <td id=\"T_ab528_row13_col4\" class=\"data row13 col4\" >+6.01%</td>\n      <td id=\"T_ab528_row13_col5\" class=\"data row13 col5\" >56.0%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row14_col0\" class=\"data row14 col0\" >PEGASUS-Large</td>\n      <td id=\"T_ab528_row14_col1\" class=\"data row14 col1\" >rougeL</td>\n      <td id=\"T_ab528_row14_col2\" class=\"data row14 col2\" >0.1738</td>\n      <td id=\"T_ab528_row14_col3\" class=\"data row14 col3\" >0.1853</td>\n      <td id=\"T_ab528_row14_col4\" class=\"data row14 col4\" >+6.63%</td>\n      <td id=\"T_ab528_row14_col5\" class=\"data row14 col5\" >62.7%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row15_col0\" class=\"data row15 col0\" >PEGASUS-Large</td>\n      <td id=\"T_ab528_row15_col1\" class=\"data row15 col1\" >bertscore</td>\n      <td id=\"T_ab528_row15_col2\" class=\"data row15 col2\" >0.0000</td>\n      <td id=\"T_ab528_row15_col3\" class=\"data row15 col3\" >0.0000</td>\n      <td id=\"T_ab528_row15_col4\" class=\"data row15 col4\" >+0.00%</td>\n      <td id=\"T_ab528_row15_col5\" class=\"data row15 col5\" >0.0%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row16_col0\" class=\"data row16 col0\" >PEGASUS-Large</td>\n      <td id=\"T_ab528_row16_col1\" class=\"data row16 col1\" >summac</td>\n      <td id=\"T_ab528_row16_col2\" class=\"data row16 col2\" >0.9149</td>\n      <td id=\"T_ab528_row16_col3\" class=\"data row16 col3\" >0.6009</td>\n      <td id=\"T_ab528_row16_col4\" class=\"data row16 col4\" >-34.33%</td>\n      <td id=\"T_ab528_row16_col5\" class=\"data row16 col5\" >7.5%</td>\n    </tr>\n    <tr>\n      <td id=\"T_ab528_row17_col0\" class=\"data row17 col0\" >PEGASUS-Large</td>\n      <td id=\"T_ab528_row17_col1\" class=\"data row17 col1\" >align</td>\n      <td id=\"T_ab528_row17_col2\" class=\"data row17 col2\" >7.8015</td>\n      <td id=\"T_ab528_row17_col3\" class=\"data row17 col3\" >7.3473</td>\n      <td id=\"T_ab528_row17_col4\" class=\"data row17 col4\" >-5.82%</td>\n      <td id=\"T_ab528_row17_col5\" class=\"data row17 col5\" >21.8%</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nHMTAS-full - BERTScore Only\nResearch-Grade Pipeline: HMTAS vs. DL Models vs. Extractive Baselines\n\n- INCLUDES: BART, PEGASUS, FLAN-T5 (Direct & HMTAS)\n- INCLUDES: LexRank & TextRank (Baselines)\n- METRICS: BERTScore Only\n- HARDWARE: Multi-GPU Support (Round-Robin), Memory Optimized\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport sys\nfrom pathlib import Path\n\n# ==========================================\n# 0. CRITICAL ENVIRONMENT FIXES\n# ==========================================\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"JAX_PLATFORMS\"] = \"cpu\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"HF_HOME\"] = str(Path(\"./hf_cache\").resolve())\nPath(os.environ[\"HF_HOME\"]).mkdir(parents=True, exist_ok=True)\n\n# ---------------------------\n# Imports\n# ---------------------------\nimport time\nimport json\nimport math\nimport random\nimport logging\nimport argparse\nimport warnings\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport nltk\nfrom typing import List, Dict, Optional, Any\n\n# ---------------------------\n# Logging Setup\n# ---------------------------\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nlogger = logging.getLogger(\"hmtas_mode_c\")\nlogger.setLevel(logging.INFO)\nif not logger.handlers:\n    ch = logging.StreamHandler()\n    ch.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n    logger.addHandler(ch)\n\n# ---------------------------\n# Safe Imports\n# ---------------------------\ntry:\n    from datasets import load_dataset\nexcept ImportError: load_dataset = None\ntry:\n    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, logging as hf_logging\n    hf_logging.set_verbosity_error()\nexcept ImportError: AutoTokenizer = None\ntry:\n    from sentence_transformers import SentenceTransformer, util as st_util\nexcept ImportError: SentenceTransformer = None\ntry:\n    from bert_score import score as bert_score\nexcept ImportError: bert_score = None\n\n# Extractive Summarizers\ntry:\n    from sumy.parsers.plaintext import PlaintextParser\n    from sumy.nlp.tokenizers import Tokenizer\n    from sumy.summarizers.lex_rank import LexRankSummarizer\n    from sumy.summarizers.text_rank import TextRankSummarizer\nexcept ImportError:\n    logger.warning(\"sumy not installed. Baselines will be skipped.\")\n    LexRankSummarizer = None\n\n# ---------------------------\n# Configuration\n# ---------------------------\nclass Config:\n    dataset_samples: int = 4\n    seed = 42\n    \n    use_gpus: bool = torch.cuda.is_available()\n    devices: List[str] = [f\"cuda:{i}\" for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else [\"cpu\"]\n\n    MODEL_REG = {\n        \"english\": {\n            \"FLAN-T5-Base\":   {\"name\": \"google/flan-t5-base\", \"ctx\": 512},\n            \"BART-Large-CNN\": {\"name\": \"facebook/bart-large-cnn\", \"ctx\": 1024},\n            \"PEGASUS-Large\":  {\"name\": \"google/pegasus-large\", \"ctx\": 1024},\n        }\n    }\n\n    gen_num_beams = 4\n    gen_batch_gpu = 8\n    encoder_name = \"paraphrase-multilingual-mpnet-base-v2\"\n    nli_model_name = \"roberta-large-mnli\"\n    \n    do_bertscore = True\n    do_summac = False\n    do_align = False\n    \n    out_dir = Path(\"./hmtas_mode_c_output\")\n    cache_dir = os.environ[\"HF_HOME\"]\n\ncfg = Config()\ncfg.out_dir.mkdir(parents=True, exist_ok=True)\n\n# ---------------------------\n# Utilities\n# ---------------------------\nSEP = \"\\n\\n---\\n\\n\"\n\ndef clean_gpu():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\ndef split_sentences(text: str) -> List[str]:\n    if not isinstance(text, str): return []\n    text = text.replace(\"\\n\", \" \").strip()\n    return [s.strip() for s in text.split(\".\") if len(s.strip().split()) > 4]\n\nclass SentenceEncoder:\n    def __init__(self, model_name: str = cfg.encoder_name, device: str = \"cpu\"):\n        self.model = SentenceTransformer(model_name, device=device, cache_folder=cfg.cache_dir)\n    def encode(self, texts: List[str]):\n        if not texts: return np.zeros((0, 384))\n        return self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n\ndef pack_articles(docs: List[str], prefix: str = \"Summarize: \") -> str:\n    return (prefix + SEP.join(docs))[:4000]\n\ndef create_guided_input(docs: List[str], encoder: Optional[SentenceEncoder]) -> str:\n    if encoder is None: return pack_articles(docs)\n    all_sents = []\n    for d in docs: all_sents.extend(split_sentences(d))\n    if not all_sents: return pack_articles(docs)\n    try:\n        emb = encoder.encode(all_sents[:50]) \n        sim = np.dot(emb, emb.T)\n        centrality = sim.sum(axis=1)\n        top_idx = sorted(np.argsort(-centrality)[:5])\n        kp = \"\\n- \".join([all_sents[i] for i in top_idx])\n    except: kp = \"\\n- \".join(all_sents[:3])\n    return (f\"Key points:\\n{kp}\\n\\nTask: Summarize.\\n\\n\" + SEP.join(docs))[:4000]\n\n# ---------------------------\n# Generation (Deep Learning)\n# ---------------------------\ndef safe_generate_batch(model, tokenizer, texts, device):\n    outs = []\n    model.eval()\n    bs = cfg.gen_batch_gpu\n    vocab_limit = model.config.vocab_size if hasattr(model, \"config\") else 32000\n    for i in range(0, len(texts), bs):\n        batch = texts[i:i+bs]\n        try:\n            inputs = tokenizer(batch, truncation=True, padding=True, max_length=512, return_tensors=\"pt\").to(device)\n            inputs[\"input_ids\"][inputs[\"input_ids\"] >= vocab_limit] = tokenizer.pad_token_id\n            with torch.no_grad():\n                gen_ids = model.generate(**inputs, max_length=256, min_length=20, num_beams=2)\n            outs.extend(tokenizer.batch_decode(gen_ids, skip_special_tokens=True))\n        except Exception:\n            outs.extend([\"\"] * len(batch))\n            torch.cuda.empty_cache()\n    return outs\n\n# ---------------------------\n# Generation (Extractive Baselines)\n# ---------------------------\ndef generate_extractive(texts: List[str], method=\"lexrank\", sentences_count=3) -> List[str]:\n    if LexRankSummarizer is None: return [\"\"] * len(texts)\n    summaries = []\n    \n    if method == \"lexrank\": summarizer = LexRankSummarizer()\n    else: summarizer = TextRankSummarizer()\n        \n    for text in texts:\n        try:\n            parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n            sents = summarizer(parser.document, sentences_count)\n            summaries.append(\" \".join([str(s) for s in sents]))\n        except:\n            summaries.append(\"\")\n    return summaries\n\n# ---------------------------\n# Main Logic\n# ---------------------------\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--split\", default=\"test\")\n    parser.add_argument(\"--samples\", type=int, default=cfg.dataset_samples)\n    args, _ = parser.parse_known_args()\n\n    logger.info(f\"ğŸš€ Starting Pipeline (Samples: {cfg.dataset_samples})\")\n    logger.info(f\"   Using Devices: {cfg.devices}\")\n    \n    # 1. LOAD DATA\n    pool = [{\"texts\": [\"Text\"]*10, \"summary\": \"Sum\"}] * cfg.dataset_samples\n    if load_dataset:\n        try:\n            ds = load_dataset(\"Awesome075/multi_news_parquet\", split=args.split, cache_dir=cfg.cache_dir)\n            pool = []\n            for item in ds:\n                docs = [d.strip() for d in item[\"document\"].split(\"||||\") if len(d) > 50]\n                if docs:\n                    pool.append({\"texts\": docs, \"summary\": item[\"summary\"]})\n                    if len(pool) >= cfg.dataset_samples: break\n        except: pass\n\n    results_store = {}\n    \n    encoder = None\n    if SentenceTransformer: encoder = SentenceEncoder(device=\"cpu\")\n\n    # 2. DEEP LEARNING GEN (MULTI-GPU ROUND ROBIN)\n    model_items = list(cfg.MODEL_REG[\"english\"].items())\n    \n    for i, (m_name, m_spec) in enumerate(model_items):\n        device = cfg.devices[i % len(cfg.devices)]\n        logger.info(f\"--> Generating DL: {m_name} on {device}\")\n        \n        clean_gpu()\n        try:\n            tok = AutoTokenizer.from_pretrained(m_spec[\"name\"], cache_dir=cfg.cache_dir)\n            mod = AutoModelForSeq2SeqLM.from_pretrained(m_spec[\"name\"], cache_dir=cfg.cache_dir).to(device)\n            if cfg.use_gpus: mod.half()\n            \n            srcs = [\" \".join(p[\"texts\"]) for p in pool]\n            refs = [p[\"summary\"] for p in pool]\n            \n            ins_d = [pack_articles(p[\"texts\"]) for p in pool]\n            ins_h = [create_guided_input(p[\"texts\"], encoder) for p in pool]\n            \n            preds_d = safe_generate_batch(mod, tok, ins_d, device)\n            preds_h = safe_generate_batch(mod, tok, ins_h, device)\n            \n            results_store[m_name] = { \"src\": srcs, \"ref\": refs, \"pred_d\": preds_d, \"pred_h\": preds_h }\n            del mod; clean_gpu()\n        except Exception as e:\n            logger.error(f\"{m_name} failed: {e}\")\n\n    # 3. BASELINE GEN (LexRank/TextRank)\n    if LexRankSummarizer:\n        logger.info(\"--> Generating Baselines...\")\n        srcs_full = [\" \".join(p[\"texts\"]) for p in pool]\n        refs = [p[\"summary\"] for p in pool]\n        \n        lex_preds = generate_extractive(srcs_full, \"lexrank\")\n        text_preds = generate_extractive(srcs_full, \"textrank\")\n        \n        results_store[\"Baseline-LexRank\"] = { \"src\": srcs_full, \"ref\": refs, \"pred_d\": lex_preds, \"pred_h\": lex_preds }\n        results_store[\"Baseline-TextRank\"] = { \"src\": srcs_full, \"ref\": refs, \"pred_d\": text_preds, \"pred_h\": text_preds }\n\n    # 4. EVALUATION - BERTScore Only\n    logger.info(\"Starting BERTScore Evaluation...\")\n    \n    eval_device = cfg.devices[0]\n    \n    bertscore_results = {}\n\n    for m_name, res in results_store.items():\n        logger.info(f\"Evaluating {m_name}...\")\n        \n        loop_modes = [\"d\", \"h\"] if \"Baseline\" not in m_name else [\"d\"]\n        \n        for mode in loop_modes:\n            preds = res[f\"pred_{mode}\"]\n            refs = res[\"ref\"]\n            \n            bs_scores = [0.0]*len(preds)\n            if cfg.do_bertscore and bert_score:\n                try:\n                    clean_gpu()\n                    _, _, F = bert_score(preds, refs, lang=\"en\", device=eval_device, verbose=False, batch_size=4)\n                    bs_scores = F.tolist()\n                    clean_gpu()\n                    \n                    mean_score = np.mean(bs_scores)\n                    logger.info(f\"  {m_name}_{mode}: Mean BERTScore = {mean_score:.4f}\")\n                    \n                    bertscore_results[f\"{m_name}_{mode}\"] = {\n                        'mean': mean_score,\n                        'std': np.std(bs_scores),\n                        'min': np.min(bs_scores),\n                        'max': np.max(bs_scores),\n                        'scores': bs_scores\n                    }\n                except Exception as e:\n                    logger.error(f\"  BERTScore failed for {m_name}_{mode}: {e}\")\n\n    # Display Results\n    logger.info(\"\\n\" + \"=\" * 70)\n    logger.info(\"BERTSCORE RESULTS\")\n    logger.info(\"=\" * 70)\n    \n    summary_data = []\n    for name, metrics in sorted(bertscore_results.items()):\n        summary_data.append({\n            'Model': name,\n            'Mean': f\"{metrics['mean']:.4f}\",\n            'Std': f\"{metrics['std']:.4f}\",\n            'Min': f\"{metrics['min']:.4f}\",\n            'Max': f\"{metrics['max']:.4f}\"\n        })\n    \n    summary_df = pd.DataFrame(summary_data)\n    print(summary_df.to_string(index=False))\n    \n    logger.info(\"\\n\" + \"=\" * 70)\n    logger.info(\"BASE vs HMTAS COMPARISON\")\n    logger.info(\"=\" * 70)\n    \n    models = {}\n    for name, metrics in bertscore_results.items():\n        parts = name.split('_')\n        if len(parts) >= 2:\n            model_name = '_'.join(parts[:-1])\n            mode = parts[-1]\n            if model_name not in models:\n                models[model_name] = {}\n            models[model_name][mode] = metrics['mean']\n    \n    for model_name, modes in sorted(models.items()):\n        print(f\"\\n{model_name}:\")\n        if 'd' in modes:\n            print(f\"  Base:  {modes['d']:.4f}\")\n        if 'h' in modes:\n            print(f\"  HMTAS: {modes['h']:.4f}\")\n        if 'd' in modes and 'h' in modes:\n            gain = ((modes['h'] - modes['d']) / modes['d']) * 100\n            print(f\"  Gain:  {gain:+.2f}%\")\n    \n    logger.info(\"\\n\" + \"=\" * 70)\n    \n    comparison_data = []\n    for model_name, modes in sorted(models.items()):\n        if 'd' in modes and 'h' in modes:\n            gain = ((modes['h'] - modes['d']) / modes['d']) * 100\n            comparison_data.append({\n                'Model': model_name,\n                'Base': f\"{modes['d']:.4f}\",\n                'HMTAS': f\"{modes['h']:.4f}\",\n                'Gain (%)': f\"{gain:+.2f}%\",\n                'Winner': 'HMTAS' if modes['h'] > modes['d'] else 'Base'\n            })\n    \n    if comparison_data:\n        comparison_df = pd.DataFrame(comparison_data)\n        print(comparison_df.to_string(index=False))\n    \n    clean_gpu()\n    logger.info(f\"\\nâœ… Done!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}